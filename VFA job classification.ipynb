{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40613757",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json \n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b85802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# create logger\n",
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c563d2",
   "metadata": {},
   "source": [
    "# Data Loading and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9da44f",
   "metadata": {},
   "source": [
    "### Load Training Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9bcad0-29f3-41af-93f2-808cea92aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "import os\n",
    "\n",
    "DB_USERNAME = os.environ.get('DB_USERNAME')\n",
    "DB_PASSWORD = os.environ.get('DB_PASSWORD')\n",
    "DB_NAME = os.environ.get('DB_NAME')\n",
    "DB_HOST = os.environ.get('DB_HOST')\n",
    "\n",
    "client = MongoClient(host=DB_HOST,port=27017,username=DB_USERNAME,password=DB_PASSWORD,authSource=DB_NAME)\n",
    "database = client[DB_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87d911-02af-424a-9183-9a5a524a587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database.iFActJobs.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f349fc-51b7-45ff-9e0a-ef1ba885d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_list = client.list_database_names()\n",
    "print(db_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d47081-dc13-41f5-904a-ba0e6c73dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iFActJobs_df=pd.DataFrame(database.iFActJobs.find({'Projectnr':{'$regex':'....[AS]'}}))\n",
    "#iFActJobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afd3b4-bfad-4972-9546-ecb84ec44485",
   "metadata": {},
   "outputs": [],
   "source": [
    "iFActJobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ece5dd-bc17-456d-998c-3b2fbad99e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "iFActPredMulti_df = pd.DataFrame(database.iFActPredMulti.find({'Projectnr':{'$regex':'....[AS]'}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77017e5-5c64-415f-a812-19e9994c5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iFActPredMulti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4322eac-91e8-4e91-95a3-836a0fe3fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jobs:\",iFActJobs_df.shape)\n",
    "print(\"Labels:\",iFActPredMulti_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca894de8-dc68-4926-9fa5-cee4e82281ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "iFActPredMulti_labelled_df=iFActPredMulti_df[iFActPredMulti_df.Emean==1]\n",
    "iFActPredMulti_labelled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df6b67-ed73-4b8e-9941-704b694f7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iFActPredMulti_df.Emean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199a440-1331-480e-97a7-8d05baff70d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# merge to get the labeled training data\n",
    "FAJobTrainingData_df=pd.merge(iFActPredMulti_labelled_df, iFActJobs_df, left_on='Projectnr',right_on='Projectnr', suffixes=[None,'_2'])\n",
    "FAJobTrainingData_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34592c",
   "metadata": {},
   "source": [
    "### Load Unlabelled Data from Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all ifact data\n",
    "\n",
    "import cx_Oracle\n",
    "import platform\n",
    "\n",
    "# This is the path to the ORACLE client files\n",
    "lib_dir = r\".\\instantclient_21_12\"\n",
    "\n",
    "# Diagnostic output to verify 64 bit arch and list files\n",
    "print(\"ARCH:\", platform.architecture())\n",
    "print(\"FILES AT lib_dir: \")\n",
    "\n",
    "try:\n",
    "    cx_Oracle.init_oracle_client(lib_dir=lib_dir)\n",
    "except Exception as err:\n",
    "    print(\"Error connecting: cx_Oracle.init_oracle_client()\")\n",
    "    print(err)\n",
    "    \n",
    "# Test to see if the cx_Oracle is recognized\n",
    "print('Oracle version: ', cx_Oracle.version)   \n",
    "\n",
    "# This fails for me at this point but will succeed after the solution described below\n",
    "cx_Oracle.clientversion() \n",
    "\n",
    "oracle_config = {\n",
    "    'username' : 'reader',\n",
    "    'password' : 'reader',\n",
    "    'dsn' : \"ifact.muc.infineon.com\",\n",
    "}\n",
    "\n",
    "def oracle_conn(user, password, dsn):    \n",
    "    oracle_conn = cx_Oracle.connect( user = user,\n",
    "                                     password = password, \n",
    "                                     dsn = dsn)\n",
    "    return oracle_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cabe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query samples\n",
    "job_site = 'MC'\n",
    "query_smpl = \"\"\"\n",
    "SELECT DISTINCT JOB_ID, JOB_PROJECT_NUMBER, JOB_PRODUCT, JOB_SUMMARY, JOB_COMMENT \n",
    "FROM READER.V_JOB_TEXTS\n",
    "WHERE JOB_PROJECT_NUMBER LIKE '{job_site}%'\n",
    "ORDER BY dbms_random.value\n",
    "OFFSET 0 ROWS FETCH NEXT 30000 ROWS ONLY\n",
    "\"\"\"\n",
    "ifact_df = pd.read_sql(query_smpl.format(job_site = job_site), \n",
    "                  con = oracle_conn(user=oracle_config['username'], \n",
    "                                    password=oracle_config['password'], \n",
    "                                    dsn=oracle_config['dsn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc759ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unlabelled data\n",
    "labelled_job_ids = list(set(FAJobTrainingData_df['Projectnr']))\n",
    "unlabelled_df = ifact_df[~ifact_df['JOB_PROJECT_NUMBER'].isin(labelled_job_ids)]\n",
    "unlabelled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a4fa42",
   "metadata": {},
   "source": [
    "### Preprocess The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982209f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    Language.factory(\"language_detector\", func=LanguageDetector())\n",
    "    nlp.add_pipe('language_detector', last=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "# text = 'This is an english text.'\n",
    "# print(doc._.language)\n",
    "\n",
    "\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        if text is not None:\n",
    "            doc = nlp(text)\n",
    "            return doc._.language['language']\n",
    "        else:\n",
    "            return 'en'\n",
    "    except Exception as e:\n",
    "        return 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    else:\n",
    "        # Remove spaces/tabs/newlines\n",
    "        processed_text = re.sub('\\s+',' ', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        processed_text = processed_text.replace(\"#\", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"=\", \"\")\n",
    "        # processed_text = processed_text.replace(\"&\", \"and\")\n",
    "\n",
    "        return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_training_data(training_df):\n",
    "    # print(training_data.info())\n",
    "\n",
    "    # Drop duplicated rows\n",
    "    # preprocess_training_data = training_df.applymap(lambda x: str(x) if not pd.isna(x) else '')\n",
    "    # preprocess_training_data = training_df.drop_duplicates().reset_index(drop=True)\n",
    "    # print('Drop duplicated rows: ', len(training_df))\n",
    "    \n",
    "    # Preprocess the text  \n",
    "    print('Preprocess the text data')\n",
    "    training_df['Processed_JobComment'] = training_df['JobComment'].apply(preprocess_input_text)\n",
    "    training_df['Processed_JobSummary'] = training_df['JobSummary'].apply(preprocess_input_text)\n",
    "\n",
    "    # split the electrical label list \n",
    "    # print('Split the electrical label list')\n",
    "    # training_df['ElPred_List'] = training_df['ElPred'].apply(split_label)\n",
    "\n",
    "    # detect the languages\n",
    "    print('Detect the languages of job summary and job comments')\n",
    "    training_df['Processed_JobComment_Lang'] = training_df['Processed_JobComment'].apply(detect_lang)\n",
    "    training_df['Processed_JobSummary_Lang'] = training_df['Processed_JobSummary'].apply(detect_lang)\n",
    "\n",
    "    return training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_df = preprocess_training_data(FAJobTrainingData_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_df[preprocessed_training_df['JobComment_Lang'] == 'de'][['JobComment', 'JobComment_Lang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c0a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_unlabelled_data(unlabelled_df):\n",
    "    # print(training_data.info())\n",
    "    \n",
    "    # Preprocess the text  \n",
    "    print('Preprocess the text data')\n",
    "    unlabelled_df['Processed_JobComment'] = unlabelled_df['JOB_COMMENT'].apply(preprocess_input_text)\n",
    "    unlabelled_df['Processed_JobSummary'] = unlabelled_df['JOB_SUMMARY'].apply(preprocess_input_text)\n",
    "\n",
    " \n",
    "    # detect the languages\n",
    "    print('Detect the languages of job summary and job comments')\n",
    "    unlabelled_df['Processed_JobComment_Lang'] = unlabelled_df['Processed_JobComment'].apply(detect_lang)\n",
    "    unlabelled_df['Processed_JobSummary_Lang'] = unlabelled_df['Processed_JobSummary'].apply(detect_lang)\n",
    "\n",
    "    return unlabelled_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f540d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_unlabelled_df = preprocess_unlabelled_data(unlabelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b381b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_training_df = preprocessed_training_df.rename(columns={\"Processed_JobSummary_Lang\":\"JobSummary_Lang\", \"Processed_JobComment_Lang\": \"JobComment_Lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba585e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_unlabelled_df = preprocessed_unlabelled_df.rename(columns={\"Processed_JobSummary_Lang\":\"JobSummary_Lang\", \"Processed_JobComment_Lang\": \"JobComment_Lang\"})\n",
    "\n",
    "preprocessed_unlabelled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_training_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840951eb",
   "metadata": {},
   "source": [
    "# Load the Electrical Failure Labels from Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff915d-3cbc-489a-8067-4e2a0fdc3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "# Prod\n",
    "prod_config = {\n",
    "    \"uri\": os.environ.get('NEO4J_URI'),\n",
    "    \"user\": os.environ.get('NEO4J_USER'),\n",
    "    \"pwd\": os.environ.get('NEO4J_PWD')\n",
    "}\n",
    "\n",
    "graph = Graph(prod_config['uri'], auth=(prod_config['user'], prod_config['pwd']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec2fa0-2626-4953-ad63-42a2664df366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_elf(graph):\n",
    "    # get all Electrical Nodes in Ontology\n",
    "    query=\"Match (n:ElectricalFailure) return n\"\n",
    "    res=graph.run(query)\n",
    "    ElFaults= [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation']] for record in res]\n",
    "    elf_df = pd.DataFrame(ElFaults, columns=['Name', 'Keywords', 'Comment', 'Explanation'])\n",
    "    elf_df['Parsed_Label'] = elf_df['Name'].apply(parse_elf_name)\n",
    "    return elf_df\n",
    "\n",
    "def parse_elf_name(label): \n",
    "    prefix = 'ElFault'\n",
    "    if label.startswith(prefix):\n",
    "        return label[len(prefix):].lower()\n",
    "    return label.lower()\n",
    "    \n",
    "def find_elf_groups(graph):\n",
    "    # get all parent Electrical Failure Nodes \n",
    "    query=\"\"\"\n",
    "    Match (parent)<-[:is_a]-(n)\n",
    "    WHERE parent.OntoName = \"ElectricalFailure\"\n",
    "    return n\"\"\"\n",
    "    res=graph.run(query)\n",
    "    ElFaults= [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation']] for record in res]\n",
    "    elf_df = pd.DataFrame(ElFaults, columns=['Name', 'Keywords', 'Comment', 'Explanation'])\n",
    "    elf_df['Parsed_Label'] = elf_df['Name'].apply(parse_elf_name)\n",
    "    return elf_df\n",
    "\n",
    "\n",
    "def find_elf_children(graph, elf_groups):\n",
    "    # get all Electrical Nodes in Ontology \n",
    "    query=f\"\"\"\n",
    "    Match (parent)<-[:is_a]-(n)\n",
    "    WHERE parent.OntoName IN {str(elf_groups)}\n",
    "    return n\"\"\"\n",
    "    res=graph.run(query)\n",
    "    ElFaults= [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation']] for record in res]\n",
    "    elf_df = pd.DataFrame(ElFaults, columns=['Name', 'Keywords', 'Comment', 'Explanation'])\n",
    "    elf_df['Parsed_Label'] = elf_df['Name'].apply(parse_elf_name)\n",
    "    return elf_df\n",
    "\n",
    "\n",
    "def find_elf_parents(graph, elf_children):\n",
    "    # get all Electrical Nodes in Ontology\n",
    "    query=f\"\"\"\n",
    "    MATCH (root {{OntoName: 'ElectricalFailure'}})<-[:is_a]-(n)<-[:is_a]-(children)\n",
    "    WHERE children.OntoName IN {str(elf_children)}\n",
    "    RETURN n\"\"\"\n",
    "    res=graph.run(query)\n",
    "    ElFaults= [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation']] for record in res]\n",
    "    elf_df = pd.DataFrame(ElFaults, columns=['Name', 'Keywords', 'Comment', 'Explanation'])\n",
    "    elf_df['Parsed_Label'] = elf_df['Name'].apply(parse_elf_name)\n",
    "    return elf_df.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "\n",
    "def load_elf_df(graph):\n",
    "    query = \"\"\"\n",
    "    Match (parent)<-[:is_a]-(n)\n",
    "    WHERE parent.OntoName = \"ElectricalFailure\"\n",
    "    return n\"\"\"\n",
    "    res=graph.run(query)\n",
    "    elf_parents = [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation'], \n",
    "        parse_elf_name(record[\"n\"]['Name']), 'Parent', None] for record in res if record[\"n\"]['Name'] != 'Probing']\n",
    "   \n",
    "    elf_children_all = []\n",
    "    # elf_parents = elf_parents[elf_parents['']]\n",
    "\n",
    "    for i, parent in enumerate(elf_parents):\n",
    "        # get all children nodes\n",
    "        query=f\"\"\"\n",
    "        Match (parent)<-[:is_a]-(n)\n",
    "        WHERE parent.OntoName = \"{parent[0]}\"\n",
    "        return n\"\"\"\n",
    "        res=graph.run(query)\n",
    "        elf_children = [[record[\"n\"]['Name'], record[\"n\"]['Keywords'], record[\"n\"]['comment'], record[\"n\"]['explanation'], \n",
    "            parse_elf_name(record[\"n\"]['Name']),  'Child', parent[4], None] for record in res]\n",
    "        elf_parents[i].append([child[4] for child in elf_children])\n",
    "        elf_children_all.extend(elf_children)\n",
    "    elf_parents_df = pd.DataFrame(elf_parents, columns=['Name', 'Keywords', 'Comment', 'Explanation', 'Parsed_Label', 'Type', 'Parent', 'Children'])\n",
    "    elf_children_df = pd.DataFrame(elf_children_all, columns=['Name', 'Keywords', 'Comment', 'Explanation', 'Parsed_Label', 'Type', 'Parent', 'Children'])\n",
    "    \n",
    "    elf_df = pd.concat([elf_parents_df, elf_children_df], axis=0, ignore_index=True)\n",
    "\n",
    "    return elf_df, elf_parents_df, elf_children_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445355a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df, elf_parents_df, elf_children_df = load_elf_df(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c1fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elf_parents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4496091",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20463d",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7ee4c",
   "metadata": {},
   "source": [
    "### Load & Process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a383a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find the matched label\n",
    "def match_elf_label(label, elf_df):\n",
    "    row = elf_df[elf_df['Parsed_Label'].str.lower() == label.lower()]\n",
    "    return row\n",
    "# label_counts = preprocessed_training_df['ElPred_List'].explode().value_counts()x\n",
    "\n",
    "def find_valid_labels(training_df, elf_df):\n",
    "    label_counts = training_df['ElPred_List'].explode().value_counts()\n",
    "\n",
    "    valid_labels = []\n",
    "    invalid_labels = []\n",
    "    for i, label in enumerate(label_counts.index):\n",
    "        matched_label =  match_elf_label(label, elf_df)\n",
    "        if matched_label.empty: \n",
    "            invalid_labels.append(label)\n",
    "            # print('#', i, label, 'label not found')\n",
    "        else:\n",
    "            valid_labels.append(label)\n",
    "            # print('#', i, label, 'label found')\n",
    "    print(len(valid_labels), valid_labels)\n",
    "    print(len(invalid_labels), invalid_labels)\n",
    "    return valid_labels\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcbbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SET_COLS = [\n",
    "    \"Projectnr\",  \n",
    "    'JobSummary', \n",
    "    'JobComment', \n",
    "    'Processed_JobComment', \n",
    "    'Processed_JobSummary', \n",
    "    'Processed_JobComment_Lang',\n",
    "    'Processed_JobSummary_Lang', \n",
    "    'TechGroup',\n",
    "    'TechGroup_2',\n",
    "    \"ElPred\",\n",
    "    ]\n",
    "\n",
    "# Split the label list\n",
    "def split_label(label):\n",
    "    # split the text by ';' and remove any empty strings or whitespace-only strings\n",
    "    label = label.replace('.', '') \n",
    "    return [item.strip() for item in label.split(';') if item.strip()]\n",
    "\n",
    "\n",
    "def filter_samples_by_language(df, cols, language='en'):\n",
    "    mask = None\n",
    "    for col in cols:\n",
    "        condition = df[col] == language \n",
    "        if mask is None:\n",
    "            mask = condition\n",
    "        else:\n",
    "            mask &= condition  \n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def validate_label(parent_label, child_label, elf_df):\n",
    "    # remove the non electrical failure\n",
    "    # if parent_label == 'not_applicable':\n",
    "    #     return False\n",
    "    \n",
    "    # remove the labels who do not follow the ontology rules\n",
    "    if parent_label is None:\n",
    "        return False\n",
    "    elif (child_label is not None) and (child_label not in list(elf_df.loc[elf_df['Parsed_Label'] == parent_label, 'Children'].iloc[0])):\n",
    "        return False\n",
    "    return True\n",
    "   \n",
    "    \n",
    "def format_tech_group(label):\n",
    "    if not pd.isna(label):\n",
    "        prefix = 'TechGroup'\n",
    "        if label.startswith(prefix):\n",
    "            return label\n",
    "        return prefix+label\n",
    "    return label\n",
    "\n",
    "\n",
    "def format_labels(label, parents_df, children_df):\n",
    "    label_list = split_label(label.lower())\n",
    "    parent_label = None\n",
    "    child_label = None\n",
    "    \n",
    "    if len(label_list) == 1:\n",
    "        if label_list[0] in list(parents_df['Parsed_Label']):\n",
    "            parent_label = label_list[0].lower()\n",
    "            child_label = None\n",
    "    \n",
    "        elif label_list[0] in list(children_df['Parsed_Label']):\n",
    "            parent_label = children_df.loc[children_df['Parsed_Label'] == label_list[0], 'Parent'].iloc[0].lower()\n",
    "            child_label = label_list[0] .lower()\n",
    "\n",
    "    elif len(label_list) == 2:\n",
    "        parent_label = label_list[0].lower()\n",
    "        child_label = None if label_list[1].lower() == 'none' else label_list[1].lower()\n",
    "   \n",
    "    # Combined label\n",
    "    if parent_label == None:\n",
    "        formatted_label = ''\n",
    "    elif child_label == None:\n",
    "        formatted_label = parent_label\n",
    "    else:\n",
    "        formatted_label = parent_label+';'+child_label\n",
    "    return [parent_label, child_label, formatted_label]\n",
    "\n",
    "\n",
    "def process_dataset(df, config):\n",
    "    # Drop duplicates\n",
    "    processed_training_data = df.drop_duplicates(\n",
    "        subset=[\"Projectnr\",  config['target_field']] + config['input_fields'])[TRAIN_SET_COLS]\n",
    "    \n",
    "    processed_training_data['ElPred_List'] = processed_training_data['ElPred'].apply(split_label)\n",
    "    \n",
    "\n",
    "    # Filter the text in English\n",
    "    processed_training_data = filter_samples_by_language(processed_training_data, [col+ '_Lang'for col in config['input_fields']], 'en')\n",
    "\n",
    "    # Only include the jobs with valid tech group\n",
    "    # processed_training_data = processed_training_data.dropna(subset=['TechGroup'])\n",
    "    processed_training_data['Parsed_TechGroup'] = processed_training_data['TechGroup'].apply(format_tech_group)\n",
    "\n",
    "    processed_training_data[['Parent_Label', 'Child_Label', 'Formatted_Label']] = pd.DataFrame(\n",
    "        processed_training_data['ElPred'].apply(lambda label: format_labels(label, elf_parents_df, elf_children_df)).tolist(), index=processed_training_data.index)\n",
    "    \n",
    "    processed_training_data['Is_Valid_Label'] = processed_training_data.apply(lambda row: validate_label(row['Parent_Label'], row['Child_Label'], elf_df), axis=1)\n",
    "    processed_training_data = processed_training_data[processed_training_data['Is_Valid_Label'] == True]\n",
    "\n",
    "    # Only include the jobs with valid labels\n",
    "    # processed_training_data = validate_labels(processed_training_data, elf_df)\n",
    "    return processed_training_data.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_full_dataset(config):\n",
    "    # Load the training data\n",
    "    training_data_path = os.path.join(config['training_data_folder'], config['training_data_name'])\n",
    "    print(f'Load The Training Data Form {training_data_path}')\n",
    "    training_data = pd.read_pickle(training_data_path)\n",
    "\n",
    "    # Preprocess the data\n",
    "    print(f'Preprocess The Training Data')\n",
    "    processed_training_data = process_dataset(training_data, config)\n",
    "    \n",
    "    return processed_training_data\n",
    "    \n",
    "\n",
    "def select_samples_from_label_groups(df, config):\n",
    "    \"\"\"Select the samples from each label\n",
    "\n",
    "    Returns:\n",
    "        sample_set, val_set\n",
    "    \"\"\"\n",
    "    if len(df) < config['training_data_size']:\n",
    "        raise ValueError(\"Not enough remaining data for non-intersecting sample sets\")\n",
    "\n",
    "    # Group the full dataset by labels\n",
    "    # expanded_df = df.explode('ElPred_List').reset_index(drop=True)\n",
    "    # grouped_df = df.groupby([config['target_field']])\n",
    "    grouped_df = df.groupby(['Formatted_Label'])\n",
    "    samples_per_group = config['training_data_size'] // len(grouped_df)\n",
    "    remaining_samples_num = config['training_data_size'] % len(grouped_df)\n",
    "    # select the samples from each group\n",
    "    \n",
    "    samples_df = pd.DataFrame()\n",
    "    for fault in grouped_df.groups.keys():\n",
    "        fault_df = grouped_df.get_group(fault)\n",
    "        if len(fault_df) < samples_per_group:\n",
    "            remaining_samples_num+=samples_per_group-len(fault_df)\n",
    "            sample_sets = fault_df\n",
    "        else:\n",
    "            sample_sets = fault_df.sample(samples_per_group, random_state=config['random_state'])\n",
    "        samples_df = pd.concat([samples_df, sample_sets], axis=0) \n",
    "    \n",
    "    if remaining_samples_num > 0:\n",
    "        remaining_indices = [idx for idx in df.index if idx not in samples_df.index]\n",
    "        random.seed(42)\n",
    "        remaining_selected = random.sample(remaining_indices, remaining_samples_num)\n",
    "        remaining_sample_sets = df.loc[remaining_selected]\n",
    "        samples_df = pd.concat([samples_df, remaining_sample_sets], axis=0)\n",
    "    \n",
    "    # Find unseen data\n",
    "    remaining_indices = [idx for idx in df.index if idx not in samples_df.index]\n",
    "    remaining_df = df.loc[remaining_indices]\n",
    "    label_counts = samples_df['Formatted_Label'].value_counts()\n",
    "    print(label_counts)\n",
    "    print('Total number of classes: ', len(label_counts))\n",
    "\n",
    "    return samples_df.reset_index(drop=True), remaining_df\n",
    "\n",
    "\n",
    "def create_non_intersecting_sample_sets(df, sample_size, set_num):\n",
    "    if sample_size * set_num > len(df):\n",
    "        raise ValueError(\"n * k should not exceed the length of the dataframe\")\n",
    "\n",
    "    sample_sets = []\n",
    "    indices = list(df.index)\n",
    "    random.seed(42)  # Set the random seed for reproducibility\n",
    "\n",
    "    for _ in range(set_num):\n",
    "        sample_indices = random.sample(indices, sample_size)\n",
    "        sample_sets.append(df.loc[sample_indices].reset_index(drop=True))\n",
    "        indices = [idx for idx in indices if idx not in sample_indices]\n",
    "    \n",
    "    # Find unseen data\n",
    "    remaining_df = df.loc[indices]\n",
    "    return sample_sets, remaining_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9db6c",
   "metadata": {},
   "source": [
    "### Check Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171154f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset quality\n",
    "def evaluate_dataset_quality(df, text_cols, lang_cols = None):\n",
    "    total_len = len(df)\n",
    "    # Calculate empty value rate for each field\n",
    "    # empty_value_rate = df[text_cols].isnull().mean()\n",
    "    empty_value_rate = [df[col].isnull().sum() + (df[col] == '').sum()/total_len for col in text_cols]\n",
    "\n",
    "    # Calculate token length for each field (split by white space)\n",
    "    token_length = df[text_cols].apply(lambda col: col.str.split().str.len().mean())\n",
    "    \n",
    "    # Detecting the languages\n",
    "    lang_cols =  [col+ '_Lang'for col in text_cols] if lang_cols == None else lang_cols\n",
    "    de_count = [(df[col] == 'de').sum() for col in lang_cols]\n",
    "    en_count = [ total_len- count for count in de_count]\n",
    "    en_ratio = [count/total_len for count in en_count]\n",
    "    # en_count = [len(df) - count for count in de_count]\n",
    "\n",
    "    # Combine the results into a DataFrame\n",
    "    quality_metrics = pd.DataFrame({\n",
    "        'Empty Value Rate': empty_value_rate, \n",
    "        'Token Length': token_length,\n",
    "        'DE Count': de_count,\n",
    "        'EN Count': en_count,\n",
    "        'EN Ratio':en_ratio \n",
    "        })\n",
    "\n",
    "    return quality_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d27c8",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f083f",
   "metadata": {},
   "source": [
    "### Aleph Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477118d5-4adf-45d4-8310-ac397e91912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_TOKEN = os.environ.get('ALPHA_TOKEN')\n",
    "ALPHA_URL = os.environ.get('ALPHA_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_completion(prompt, config):\n",
    "    payload = json.dumps({\n",
    "    \"model\": config['model_name'],\n",
    "    \"prompt\": prompt,\n",
    "    \"maximum_tokens\": config['max_tokens'],\n",
    "    \"temperature\": config['temperature'],\n",
    "    \"stop_sequences\": [\"###\", \"\\n\"]\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'cookie': 'token='+ALPHA_TOKEN\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", ALPHA_URL, headers=headers, data=payload, verify=False)\n",
    "    res = json.loads(response.text)\n",
    "    if 'error' in res.keys():\n",
    "        return res\n",
    "    else:\n",
    "        return res['completions'][0]['completion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f39d0",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25982500",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mistral API 'Mistral-7B-Instruct'\n",
    "OPENAI_URL = os.environ.get('OPENAI_URL')\n",
    "\n",
    "def mistral_completion(prompt, config):\n",
    "    # print(create_prompt_mistral(input))\n",
    "    payload = json.dumps({\n",
    "    \"model\": config['model_name'],\n",
    "    \"prompt\": prompt,\n",
    "    \"max_tokens\": config['max_tokens'],\n",
    "    \"temperature\": config['temperature'],\n",
    "    })\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\"POST\", OPENAI_URL, headers=headers, data=payload, verify=False)\n",
    "    res = json.loads(response.text)\n",
    "    if response.status_code != 200:\n",
    "        return res\n",
    "    else:\n",
    "        return res['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e073d5",
   "metadata": {},
   "source": [
    "### IFXGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mistral API 'Mistral-7B-Instruct'\n",
    "gpt4ifx_url = os.environ.get('GPT4IFX_URL')\n",
    "gpt4ifx_user=os.environ.get('GPT4IFX_USER')\n",
    "gpt4ifx_password=os.environ.get('GPT4IFX_PASSWORD')\n",
    "\n",
    "def mixtral_completion(prompt, config):\n",
    "    # print(create_prompt_mistral(input))\n",
    "    payload = json.dumps({\n",
    "    \"model\": config['model_name'],\n",
    "    \"messages\": [{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }],\n",
    "    \"n\": 1,\n",
    "    \"max_tokens\": config['max_tokens'],\n",
    "    \"temperature\": config['temperature'],\n",
    "    })\n",
    "    \n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.request(\"POST\", gpt4ifx_url, headers=headers, auth=(gpt4ifx_user,gpt4ifx_password), data=payload, verify=False)\n",
    "    res = json.loads(response.text)\n",
    "    if response.status_code != 200:\n",
    "        return res\n",
    "    else:\n",
    "        return res['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48f731",
   "metadata": {},
   "source": [
    "### Few Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff0c8f",
   "metadata": {},
   "source": [
    "##### Few-shots sample selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3365d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_manual_samples(df, sample_indexes):\n",
    "    return df.iloc[sample_indexes]\n",
    "\n",
    "\n",
    "def select_random_samples(df, shots_num, random_state):\n",
    "    \"\"\"Function to randomly select the random samples\n",
    "    \"\"\"\n",
    "    # randomly select sample from dataframe\n",
    "    samples = df.sample(n=shots_num, random_state=random_state) if len(df) > shots_num else df\n",
    "    return samples\n",
    "\n",
    "\n",
    "def select_similar_samples(input_text, df, tfidf_matrix, vectorizer, shots_num):\n",
    "    \"\"\"Function to select similar text examples\n",
    "    \"\"\"\n",
    "    df['Similarity'] =  cosine_similarity(vectorizer.transform([input_text]), tfidf_matrix)[0]\n",
    "    selected_samples = df.nlargest(shots_num, 'Similarity')\n",
    "    return selected_samples\n",
    "\n",
    "def compute_tfidf_matrix(df, input_fields):\n",
    "    combined_texts = df[input_fields].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(combined_texts)\n",
    "    return vectorizer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec75b9",
   "metadata": {},
   "source": [
    "##### Filter elf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the possible failure modes\n",
    "def get_all_tech_groups():\n",
    "    # Get tech group\n",
    "    onto_url = os.environ.get('ONTO_URL')\n",
    "    response = requests.get(url = onto_url,verify = False)\n",
    "    res = json.loads(response.text)\n",
    "    return res['response']\n",
    "\n",
    "def filter_elf_by_tech_group(tech_group, elf_df):\n",
    "    \"\"\" Find the possible electrical fault with given tech group\n",
    "    \"\"\"\n",
    "    if tech_group == None:\n",
    "        return elf_df\n",
    "    try:\n",
    "        onto_url = os.environ.get('ONTO_URL') + f'?tech_group={tech_group}'\n",
    "        response = requests.get(url = onto_url,verify = False)\n",
    "        res = json.loads(response.text)\n",
    "        possible_elf = [label.lower() for label in json.loads(res['response']) ]\n",
    "        # print(possible_elf)\n",
    "        return elf_df[elf_df['Parsed_Label'].isin(possible_elf) | elf_df['Parent'].isin(possible_elf)]\n",
    "    except Exception as e:\n",
    "        print(f'Error getting the electrical fauilure related to {tech_group}, {e}')\n",
    "        return elf_df\n",
    "\n",
    "def filter_elf_by_keywords(input_texts, elf_df):\n",
    "    # filterd elf_df with keywords\n",
    "    # elf_df_filtered = elf_df[elf_df['Keywords'].notna()].copy()\n",
    "\n",
    "    if 'split_keywords' not in elf_df.columns:\n",
    "        if 'Keywords' in elf_df.columns:\n",
    "            valid_keywords = elf_df['Keywords'].dropna().astype(str)\n",
    "            elf_df['split_keywords'] = valid_keywords.apply(split_label)\n",
    "\n",
    "    possible_elf = []\n",
    "    found_match = False  # Add a flag variable to track if a match is found \n",
    "\n",
    "    for _, row in elf_df.iterrows(): \n",
    "        if isinstance(row['split_keywords'], list):\n",
    "            for keyword in row['split_keywords']:\n",
    "                if keyword in input_texts:\n",
    "                    possible_elf.append(row['Parsed_Label'])\n",
    "                    found_match = True\n",
    "                    break\n",
    "    # possible_elf = list()\n",
    "    filtered_elf = elf_df[\n",
    "        (elf_df['Parsed_Label'].isin(possible_elf)) | \n",
    "        (elf_df['Parent'].isin(possible_elf)) |\n",
    "        (elf_df['Children'].apply(lambda x: isinstance(x, list) and any(item in possible_elf for item in x) if x is not None else False))\n",
    "    ]\n",
    "    # If no match was found, return the filterd elf_df\n",
    "    if not found_match:\n",
    "        return elf_df\n",
    "    else:\n",
    "        return filtered_elf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecde0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5ce5c",
   "metadata": {},
   "source": [
    "#### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_labelling_prompt_alpha(shots_df, elf_df, input_text, config):\n",
    "#    task_definiation = \"\"\"Predict the parent failure mode from the following list based on the report comment. \\\n",
    "# Once the parent failure mode is predicted, select the corresponding child failure mode if applicable. \\\n",
    "# The predictions should be in the format of 'parent_label' or 'parent_label;child_label'. \\\n",
    "# If no suitable parent failure mode can be found, use 'not_applicable' as the label.\n",
    "# \"\"\"\n",
    "\n",
    "def create_labelling_prompt_alpha(shots_df, elf_df, input_text, config):\n",
    "   task_definiation = \"\"\"Predict the parent failure mode from the following list based on the report comment. \\\n",
    "Once the parent failure mode is predicted, select the corresponding child failure mode if applicable. \\\n",
    "If no suitable parent failure mode can be found, use 'not_applicable' as the parent label.\n",
    "\"\"\"\n",
    "\n",
    "   # Provide the list of the ontology\n",
    "   label_lists = 'Here is the list of parent nodes:\\n'\n",
    "   for i in range(len(elf_parents_df)):\n",
    "      ontology = elf_parents_df.iloc[i]\n",
    "      label_lists += f\"- {ontology['Parsed_Label']}:\"\n",
    "      if ontology['Keywords'] :\n",
    "         label_lists += f\" has keywords({ontology['Keywords'] }).\"\n",
    "      if ontology['Explanation']  is not None:\n",
    "         label_lists += f\" has meaning({ontology['Explanation'] }).\"\n",
    "      if ontology['Children'] is not None:\n",
    "         label_lists += f\" has children label ({ontology['Children'] }).\"\n",
    "      label_lists += '\\n'\n",
    "   \n",
    "   label_lists += 'Here is the list of child nodes:\\n'\n",
    "   \n",
    "   for i in range(len(elf_children_df)):\n",
    "      ontology = elf_children_df.iloc[i]\n",
    "      label_lists += f\"- {ontology['Parsed_Label']}:\"\n",
    "      if ontology['Keywords'] :\n",
    "         label_lists += f\" has keywords({ontology['Keywords'] }).\"\n",
    "      if ontology['Explanation']  is not None:\n",
    "         label_lists += f\" has meaning({ontology['Explanation'] }).\"\n",
    "      if ontology['Parent'] is not None:\n",
    "         label_lists += f\" has parent label({ontology['Parent'] }).\"\n",
    "      label_lists += '\\n'\n",
    "   # Insert the input text\n",
    "   input_section = f\"\"\"\n",
    "### \n",
    "Comment: {input_text}\n",
    "Failure:\"\"\"\n",
    "   \n",
    "   remaining_len = config['max_prompt_len']\\\n",
    "      - len(task_definiation.split())\\\n",
    "      - len(label_lists.split())\\\n",
    "      - len(input_section.split())\n",
    "   \n",
    "   shots_text = ''\n",
    "   # Provide few shots\n",
    "   for i in range(shots_df.shape[0]):\n",
    "        sample = shots_df.iloc[i]\n",
    "        example=f\"\"\"\n",
    "###\n",
    "Comment: {' '.join([sample[col] for col in config['input_fields']])}\n",
    "Failure: parent label: {str(sample['Parent_Label'])}, child label: {str(sample['Child_Label'])}\"\"\"\n",
    "        \n",
    "        example_len = len(example.split())\n",
    "        if example_len > remaining_len:\n",
    "            break\n",
    "        else:\n",
    "            shots_text += example\n",
    "            remaining_len -= example_len\n",
    "\n",
    "   prompt_text = task_definiation + label_lists + shots_text + input_section\n",
    "   \n",
    "   return prompt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47381f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_elf_label_list(elf_df, label_rel = False):\n",
    "    if label_rel:\n",
    "        elf_parents_df = elf_df[elf_df['Type'] == 'Parent']\n",
    "        elf_children_df = elf_df[elf_df['Type'] == 'Child']\n",
    "        \n",
    "        label_lists = 'Here is the list of parent failure mode labels:\\n'\n",
    "        for i in range(len(elf_parents_df)):\n",
    "            ontology = elf_parents_df.iloc[i]\n",
    "            label_lists += f\"- Label: {ontology['Parsed_Label']}\\n\"\n",
    "            if ontology['Keywords'] :\n",
    "                label_lists += f\" Keywords: {ontology['Keywords']}\\n\"\n",
    "            if ontology['Explanation']  is not None:\n",
    "                label_lists += f\" Explanation: {ontology['Explanation']}\\n\"\n",
    "            if ontology['Children'] is not None:\n",
    "                label_lists += f\" Children labels: {ontology['Children']}\\n\"\n",
    "            label_lists += '\\n'\n",
    "        \n",
    "        label_lists += 'Here is the list of child failure mode labels:\\n'\n",
    "        \n",
    "        for i in range(len(elf_children_df)):\n",
    "            ontology = elf_children_df.iloc[i]\n",
    "            label_lists += f\"- Label: {ontology['Parsed_Label']}\\n\"\n",
    "            if ontology['Keywords'] :\n",
    "                label_lists += f\" Keywords: {ontology['Keywords']}\\n\"\n",
    "            if ontology['Explanation']  is not None:\n",
    "                label_lists += f\" Explanation: {ontology['Explanation']}\\n\"\n",
    "            if ontology['Parent'] is not None:\n",
    "                label_lists += f\" Parent label: {ontology['Parent']}\\n\"\n",
    "            label_lists += '\\n'\n",
    "    else:\n",
    "        label_lists = ''\n",
    "        \n",
    "        for i in range(len(elf_df)):\n",
    "            ontology = elf_df.iloc[i]\n",
    "            label_lists += f\"- Label: {ontology['Parsed_Label']}\\n\"\n",
    "            if ontology['Keywords'] :\n",
    "                label_lists += f\" Keywords: {ontology['Keywords']}\\n\"\n",
    "            if ontology['Explanation']  is not None:\n",
    "                label_lists += f\" Explanation: {ontology['Explanation']}\\n\"\n",
    "            label_lists += '\\n'\n",
    "    \n",
    "    return label_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shots_text(shots_df, input_fields, target_fields, max_token_len):\n",
    "    shots_text = ''\n",
    "    for i in range(shots_df.shape[0]):\n",
    "        sample = shots_df.iloc[i]\n",
    "\n",
    "        if len(target_fields) == 1:\n",
    "            example =f\"\"\"\n",
    "Example {i + 1}:\n",
    "Job Comment: {' '.join([sample[col] for col in input_fields])}\n",
    "Failure: {str(sample[target_fields[0]])}\"\"\"\n",
    "        else:\n",
    "            example =f\"\"\"\n",
    "Example {i + 1}:\n",
    "Job Comment: {' '.join([sample[col] for col in input_fields])}\n",
    "Failure: parent label: {str(sample[target_fields[0]])}, child label: {str(sample[target_fields[1]])}\"\"\"\n",
    "\n",
    "        example_len = len(example.split())\n",
    "        if example_len > max_token_len:\n",
    "            break\n",
    "        else:\n",
    "            shots_text += example\n",
    "            max_token_len -= example_len\n",
    "    \n",
    "    return shots_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d896e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labelling_prompt_mistral(shots_df, elf_df, input_text, config, label_rel):\n",
    "    task_definiation = \"\"\"[INST]\n",
    "You are a classifier, and your task is to analyze the given 'Job Comment' and assign the appropriate the failure mode from the provided label list based on the report comment. \\\n",
    "First predict the parent failure mode from the following list based on the report comment. \\\n",
    "Once the parent failure mode is predicted, select only one corresponding child failure mode if applicable. \\\n",
    "If no suitable parent failure mode can be found, parent label should be 'not_applicable', child label should be 'None'. \\\n",
    "The Failure should always in the format of \"parent label: parent failure mode label, child label: child failure mode label\".\\\n",
    "Your output should only contain the predicted Failure in the required format and do not include any explanations.\n",
    "<<<\n",
    "\"\"\"\n",
    " \n",
    "    label_lists = 'Here is the list of failure modes:\\n'\n",
    "    label_lists += create_elf_label_list(elf_df, label_rel)\n",
    "\n",
    "    task_guidelines = f\"\"\">>>\n",
    "\n",
    "###\n",
    "Here are some examples to guide your labeling process:\n",
    "\"\"\"\n",
    "    \n",
    "    input_section = f\"\"\"\n",
    "###\n",
    "Now, please label the following job comment:\n",
    "\n",
    "Job Comment: {input_text}\n",
    "Failure:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "    remaining_len = config['max_prompt_len']\\\n",
    "      - len(task_definiation.split())\\\n",
    "      - len(label_lists.split())\\\n",
    "      - len(task_guidelines.split())\\\n",
    "      - len(input_section.split())\n",
    "    \n",
    "    \n",
    "    # Provide few shots\n",
    "    shots_text = ''\n",
    "    for i in range(shots_df.shape[0]):\n",
    "        sample = shots_df.iloc[i]\n",
    "        example =f\"\"\"\n",
    "Example {i + 1}:\n",
    "Job Comment: {' '.join([sample[col] for col in config['input_fields']])}\n",
    "Failure: parent label: {str(sample['Parent_Label'])}, child label: {str(sample['Child_Label'])}\"\"\"\n",
    "\n",
    "        example_len = len(example.split())\n",
    "        if example_len > remaining_len:\n",
    "            break\n",
    "        else:\n",
    "            shots_text += example\n",
    "            remaining_len -= example_len\n",
    "\n",
    "    prompt_text = task_definiation + label_lists + task_guidelines + shots_text + input_section\n",
    "\n",
    "    return prompt_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0b1eb",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def completion(shots_df, elf_df, input_text, config):\n",
    "    # infer the parenet node\n",
    "    if config['model_type'] == 'alpha':\n",
    "        prompt = create_labelling_prompt_alpha(shots_df, elf_df, input_text, config)\n",
    "        return alpha_completion(prompt, config), prompt\n",
    "    elif config['model_type'] == 'mistral':\n",
    "        prompt = create_labelling_prompt_mistral(shots_df, elf_df, input_text, config, True)\n",
    "        return  mistral_completion(prompt, config), prompt\n",
    "    elif config['model_type'] == 'mixtral':\n",
    "        prompt = create_labelling_prompt_mistral(shots_df, elf_df, input_text, config, True)\n",
    "        return  mixtral_completion(prompt, config), prompt\n",
    "\n",
    "\n",
    "def single_inference(input_sample, elf_df, labelled_data, config):\n",
    "    \"\"\"Function to start a single inference process\n",
    "\n",
    "    Args:\n",
    "        input_sample (dataframe): The dataframe row\n",
    "        labelled_data (dataframe): The dataframe of labelled data, which is the source of shots examples\n",
    "        config (dict): inference config\n",
    "\n",
    "    Returns:\n",
    "        Label, prediction, prompt\n",
    "    \"\"\"\n",
    "    try: \n",
    "        input_text = ' '.join([input_sample[col] for col in config['input_fields']])\n",
    "        # label = input_sample['Formatted_Label']\n",
    "\n",
    "        # select few shots\n",
    "        if config['shots_selects_method'] == 'random':\n",
    "            shots_df = select_random_samples(labelled_data, config['shots_num'], 0)\n",
    "        elif config['shots_selects_method'] == 'manual':\n",
    "            shots_df= select_manual_samples(labelled_data, config['shots_indexes'])\n",
    "        else:\n",
    "            shots_df = select_similar_samples(input_text, labelled_data, config['tfidf_matrix'], config['vectorizer'], config['shots_num'])\n",
    "\n",
    "        # filter the elf\n",
    "        if config['elf_filter'] != None:\n",
    "            filtered_elf_df = elf_df.copy(deep=True)\n",
    "            if 'keywords' in config['elf_filter']:\n",
    "                filtered_elf_df = filter_elf_by_keywords(input_text, elf_df)\n",
    "            if 'tech' in config['elf_filter']:\n",
    "                filtered_elf_df = filter_elf_by_tech_group(input_sample['TechGroup'], elf_df)\n",
    "\n",
    "        # infer\n",
    "        if config['model_type'] == 'alpha':\n",
    "            prompt = create_labelling_prompt_alpha(shots_df, filtered_elf_df, input_text, config)\n",
    "            return alpha_completion(prompt, config), prompt\n",
    "\n",
    "        elif config['model_type'] == 'mistral':\n",
    "            prompt = create_labelling_prompt_mistral(shots_df, filtered_elf_df, input_text, config, True)\n",
    "            return  mistral_completion(prompt, config), prompt\n",
    "\n",
    "        else:\n",
    "            prompt = create_labelling_prompt_mistral(shots_df, filtered_elf_df, input_text, config, True)\n",
    "            return  mixtral_completion(prompt, config), prompt\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e, ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9872bf6-1638-4c87-a060-190ee8ccbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference_two_level(input_sample, elf_df, labelled_data, config):\n",
    "    \"\"\"Function to start a single inference process\n",
    "\n",
    "    Args:\n",
    "        input_sample (dataframe): The dataframe row\n",
    "        labelled_data (dataframe): The dataframe of labelled data, which is the source of shots examples\n",
    "        config (dict): inference config\n",
    "\n",
    "    Returns:\n",
    "        Label, prediction, prompt\n",
    "    \"\"\"\n",
    "    input_text = ' '.join([input_sample[col] for col in config['input_fields']])\n",
    "    label = input_sample[config['target_field']]\n",
    "\n",
    "    # select few shots\n",
    "    if config['shots_selects_method'] == 'random':\n",
    "        shots_df = select_random_samples(labelled_data, config['shots_num'], 0)\n",
    "    elif config['shots_selects_method'] == 'manual':\n",
    "        shots_df= select_manual_samples(labelled_data, config['shots_indexes'])\n",
    "    else:\n",
    "        shots_df = select_similar_samples(input_text, labelled_data, config['tfidf_matrix'], config['vectorizer'], config['shots_num'])\n",
    "\n",
    "    # filter the elf\n",
    "    if config['elf_filter'] != None:\n",
    "        filtered_elf_df = elf_df.copy(deep=True)\n",
    "        if 'keywords' in config['elf_filter']:\n",
    "            filtered_elf_df = filter_elf_by_keywords(input_text, elf_df)\n",
    "        if 'tech' in config['elf_filter']:\n",
    "            filter_elf_by_tech_group(input_sample['Parsed_TechGroup'], elf_df)\n",
    "    \n",
    "    elf_parents_df = filtered_elf_df[filtered_elf_df['Type'] == 'Parent'] \n",
    "    elf_children_df = filtered_elf_df[filtered_elf_df['Type'] == 'Child'] \n",
    "\n",
    "    # infer\n",
    "    if config['model_type'] == 'alpha':\n",
    "        prompt = create_labelling_prompt_alpha(shots_df, elf_parents_df, input_text, config)\n",
    "        res = alpha_completion(prompt, config)\n",
    "        parents_labels = process_prediction_label(res, list(elf_parents_df['Parsed_Label']))\n",
    "        \n",
    "        # infer the child labels\n",
    "        prompt = create_labelling_prompt_alpha(shots_df, elf_children_df, input_text, config)\n",
    "        res = alpha_completion(prompt, config)\n",
    "        child_labels = process_prediction_label(res, list(elf_children_df['Parsed_Label']) )\n",
    "     \n",
    "        return parents_labels,  child_labels\n",
    "\n",
    "    elif config['model_type'] == 'mistral':\n",
    "\n",
    "        prompt = create_labelling_prompt_mistral_two_level(shots_df, elf_parents_df, input_text, config, False)\n",
    "\n",
    "        return label, mistral_completion(prompt, config), prompt\n",
    "    \n",
    "    return parent_labels, child_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634b895",
   "metadata": {},
   "source": [
    "### Post Process and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa522f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_prediction_label(label_str, ontology_list):\n",
    "    \"\"\"Remove duplicated predictions and return only the ontology\n",
    "    Args:\n",
    "        label_str (str): A string representing the labels. Each label should be separated by a semicolon.\n",
    "        label_list (list[str]): list of ontology label\n",
    "    Returns:\n",
    "        list[str]: Processed labels\n",
    "    \"\"\"\n",
    "    # print(label_str)\n",
    "    label_str = label_str.replace('.', '').replace('#', '').replace(':', '') \n",
    "    processed_labels = list(set([item.strip().lower() for item in label_str.split(';') if item.strip()]))\n",
    "    processed_labels = [label for label in processed_labels if label.lower() in list(map(str.lower, ontology_list))]\n",
    "    return list(processed_labels)\n",
    "\n",
    "def extract_prediction_labels(label_str):\n",
    "    if isinstance(label_str, str):\n",
    "        parent_label = 'not_applicable'\n",
    "        child_label = None\n",
    "        parent_label_pattern = r'[Pp]arent label:\\s*(\\w+)'\n",
    "        child_label_pattern = r'[Cc]hild label:\\s*(\\w+)'\n",
    "        parent_label_match = re.search(parent_label_pattern, label_str)\n",
    "        child_label_match = re.search(child_label_pattern, label_str)\n",
    "        \n",
    "        if parent_label_match:\n",
    "            parent_label = parent_label_match.group(1).lower()\n",
    "            if parent_label not in list(elf_parents_df['Parsed_Label']):\n",
    "                parent_label = 'not_applicable'\n",
    "            elif child_label_match:\n",
    "                child_label = child_label_match.group(1).lower()\n",
    "                if child_label not in elf_parents_df[elf_parents_df['Parsed_Label'] == parent_label]['Children'].iloc[0]:\n",
    "                    child_label = None\n",
    "\n",
    "        return parent_label, child_label\n",
    "    else:\n",
    "        return 'not_applicable', None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred, \n",
    "        labels = list(set(y_true)))\n",
    "    return cm.tolist()\n",
    "\n",
    "def plot_confusion_matrix(cm, labels):\n",
    "    sns.heatmap(cm, annot=True,fmt='d', cmap='YlGnBu', xticklabels=labels, yticklabels=labels)\n",
    "    plt.ylabel('Prediction',fontsize=12)\n",
    "    plt.xlabel('Actual',fontsize=12)\n",
    "    plt.title('Confusion Matrix',fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def get_classification_report(y_true, y_pred):\n",
    "    # print(classification_report(y_true, y_pred))\n",
    "    return classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "def plot_report_tabel(report):\n",
    "    table = []\n",
    "    for key, value in report.items():\n",
    "        if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            precision = \"{:.2%}\".format(value['precision'])\n",
    "            recall = \"{:.2%}\".format(value['recall'])\n",
    "            f1_score = \"{:.2%}\".format(value['f1-score'])\n",
    "            support = int(value['support'])  # Support value left as-is\n",
    "            table.append([key, precision, recall, f1_score, support])\n",
    "\n",
    "    # Create a DataFrame from the processed data\n",
    "    df = pd.DataFrame(table, columns=['Label', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "    # Plot the DataFrame as a table\n",
    "    fig, ax = plt.subplots(figsize=(10, 2))\n",
    "    ax.axis('off')\n",
    "    tbl = ax.table(cellText=df.values, colLabels=df.columns, loc='center')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_results(results):\n",
    "    eval_res = []\n",
    "    error_count = 0\n",
    "    perfect_match_count = 0\n",
    "    partial_match_count = 0\n",
    "\n",
    "    i = 0\n",
    "    for tgt_parent_label, tgt_child_label, pred_parent_label, pred_child_label, pred_str, prompt in results:\n",
    "        i+=1\n",
    "        match_res = ''\n",
    "        if pred_str:\n",
    "            if tgt_parent_label == pred_parent_label:\n",
    "                if tgt_child_label == pred_child_label:\n",
    "                    perfect_match_count += 1\n",
    "                    partial_match_count += 1\n",
    "                    match_res = 'perfect_match'\n",
    "                else:\n",
    "                    partial_match_count += 1\n",
    "                    match_res = 'partial_match'\n",
    "            else:\n",
    "                match_res = 'no_match'\n",
    "        else:\n",
    "            match_res = 'error'\n",
    "            error_count += 1\n",
    "\n",
    "        eval_res.append([tgt_parent_label, tgt_child_label, pred_parent_label, pred_child_label, pred_str, match_res, prompt])\n",
    "\n",
    "    total = len(results)\n",
    "\n",
    "    perfect_match_rate_all = perfect_match_count / total\n",
    "    partial_match_rate_all  = partial_match_count / total\n",
    "\n",
    "    if total - error_count != 0:\n",
    "        perfect_match_rate_valid  = perfect_match_count / (total - error_count)\n",
    "        partial_match_rate_valid  = partial_match_count / (total - error_count)\n",
    "    else:\n",
    "        perfect_match_rate_valid  = 0\n",
    "        partial_match_rate_valid  = 0\n",
    "        \n",
    "    print(f\"Total predictions: {total}\")\n",
    "    print(f\"Error predictions: {error_count}\")\n",
    "    print(f\"Valid predictions: {total - error_count}\")\n",
    "\n",
    "    # print(perfect_match_count, partial_match_count)\n",
    "    print(f\"Perfect match rate (all): {perfect_match_rate_all * 100}%\")\n",
    "    print(f\"Partial match rate (all): {partial_match_rate_all * 100}%\")\n",
    "\n",
    "    print(f\"Perfect match rate (valid): {perfect_match_rate_valid * 100}%\")\n",
    "    print(f\"Partial match rate (valid): {partial_match_rate_valid * 100}%\")\n",
    "    \n",
    "    res_df = pd.DataFrame(eval_res, columns=['Target_Parent', 'Target_Child', 'Pred_Parent',  'Pred_Child',  'Pred', 'Result', 'Prompt'])\n",
    "    y_true=res_df['Target_Parent']\n",
    "    y_pred=res_df['Pred_Parent']\n",
    "    \n",
    "    eval_matrix = {\n",
    "        'result': {\n",
    "            'Perfect match rate (all)': perfect_match_rate_all, \n",
    "            'Partial match rate (all)': partial_match_rate_all, \n",
    "            'Perfect match rate (valid)': perfect_match_rate_valid, \n",
    "            'Partial match rate (valid)': partial_match_rate_valid\n",
    "        }, \n",
    "        'report' : get_classification_report(y_true, y_pred),\n",
    "        'confusion_matrix': get_confusion_matrix(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    plot_report_tabel(eval_matrix['report'])\n",
    "    plot_confusion_matrix(eval_matrix['confusion_matrix'], list(set(y_true)))\n",
    "    return eval_matrix, res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "def save_dict_to_json(dict_data, file_path):\n",
    "    try:\n",
    "        # Remove specified fields from the dictionary\n",
    "        for field in ['tfidf_matrix', 'vectorizer']:\n",
    "            dict_data.pop(field, None)\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(dict_data, file, indent=4)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the dictionary data to JSON file: {e}\")\n",
    "        return False\n",
    "\n",
    "def save_evaluation_results(training_set, eval_matrix, res_df, config):\n",
    "    now = datetime.now() \n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")  \n",
    "\n",
    "    # Process evaluation results\n",
    "    res_df = pd.concat([training_set, res_df], axis=1) \n",
    "\n",
    "    res_name = f\"{config['model_name']}_{config['evaluation_method']}_{config['shots_selects_method']}_{config['elf_filter']}_{timestamp}\"\n",
    "    folder_path = os.path.join(config['results_folder'], res_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    file_path = os.path.join(folder_path, f'eval_results_{res_name}.xlsx')\n",
    "    res_df.to_excel(file_path, index=False, engine='xlsxwriter')\n",
    "\n",
    "    # Save config\n",
    "    save_dict_to_json(config, os.path.join(folder_path, f'config_{res_name}.json'))\n",
    "\n",
    "    # Save evaluation matrix\n",
    "    save_dict_to_json(eval_matrix, os.path.join(folder_path, f'eval_matrix_{res_name}.json'))\n",
    "    print(f'Evaluation results save to {folder_path}')\n",
    "\n",
    "\n",
    "def save_evaluation_results_k_folds(folds, eval_matrix_list, eval_res_list, config):\n",
    "    now = datetime.now() \n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")  \n",
    "    folder_path = os.path.join(config['results_folder'], f\"{config['model_name']}_{config['evaluation_method']}_{config['shots_selects_method']}_{timestamp}\")\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    res_df = pd.DataFrame()\n",
    "    eval_matrix = {}\n",
    "    for i in range(len(folds)):\n",
    "        # Process evaluation results\n",
    "        fold_res_df = pd.DataFrame(eval_res_list[i], columns=['Target', 'Pred',  'Processed_Pred', 'Result', 'Prompt'])\n",
    "        fold_res_df = pd.concat([folds[i], fold_res_df], axis=1) \n",
    "        fold_res_df['fold'] = i\n",
    "        res_df = pd.concat([res_df, fold_res_df], axis=0) \n",
    "        eval_matrix[f'fold_{i}'] = eval_matrix_list[i]\n",
    "\n",
    "    # Compute the average acc\n",
    "    eval_matrix[\"average\"] = {\n",
    "        'Perfect match rate (all)': np.mean([matrix[\"Perfect match rate (all)\"] for matrix in eval_matrix_list]),\n",
    "        'Partial match rate (all)': np.mean([matrix[\"Partial match rate (all)\"] for matrix in eval_matrix_list]), \n",
    "        'Perfect match rate (valid)': np.mean([matrix[\"Perfect match rate (valid)\"] for matrix in eval_matrix_list]), \n",
    "        'Partial match rate (valid)': np.mean([matrix[\"Partial match rate (valid)\"] for matrix in eval_matrix_list]), \n",
    "    }\n",
    "    \n",
    "    file_path = os.path.join(folder_path, f'eval_results_{timestamp}.xlsx')\n",
    "    res_df.to_excel(file_path, index=False, engine='xlsxwriter')\n",
    "\n",
    "    # Save config\n",
    "    save_dict_to_json(config, os.path.join(folder_path, f'config_{timestamp}.json'))\n",
    "\n",
    "    # Save evaluation matrix\n",
    "    save_dict_to_json(eval_matrix, os.path.join(folder_path, f'eval_matrix_{timestamp}.json'))\n",
    "    print(f'Evaluation results save to {folder_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b7ed8",
   "metadata": {},
   "source": [
    "# Start the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inference_evaluation_process(samples_df, shots_df, config):\n",
    "    \"\"\"Function to get all prediction for given datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(len(samples_df)):\n",
    "        sample = samples_df.iloc[i]\n",
    "        # try: \n",
    "        pred, prompt = single_inference(sample, elf_df, shots_df, config)\n",
    "        pred_parent, pred_child = extract_prediction_labels(pred)\n",
    "        print(i, sample['Parent_Label'], sample['Child_Label'], pred_parent, pred_child)\n",
    "        predictions.append((sample['Parent_Label'], sample['Child_Label'], pred_parent, pred_child, pred, prompt))\n",
    "        # except Exception as e:\n",
    "        #     predictions.append((sample['Parent_Label'], sample['Child_Label'], None, None, '', e))\n",
    "        #     print(f'{i} Error getting predictions: {e}')\n",
    "    return predictions\n",
    "\n",
    "def trigger_inference_evaluation_process_single(df, config):\n",
    "    # Select the sample data\n",
    "    # sample_data = processed_training_data.iloc[:training_config['training_data_size']]\n",
    "    training_set, remaining_df = select_samples_from_label_groups(df, config)\n",
    "    # Check data quality\n",
    "    print(f'Training Data Quality: ')\n",
    "    quality_results = evaluate_dataset_quality(training_set, config['input_fields'])\n",
    "    print(quality_results)\n",
    "\n",
    "    # Compute TF-IDF vectors for the combined text and existing samples\n",
    "    if config['shots_selects_method'] == 'similar':\n",
    "        vectorizer, tfidf_matrix = compute_tfidf_matrix(remaining_df, config['input_fields'])\n",
    "        config['vectorizer'] = vectorizer\n",
    "        config['tfidf_matrix'] = tfidf_matrix\n",
    "        \n",
    "    # Start the inference process\n",
    "    print(f'Start the Inference Process ')\n",
    "    predictions = inference_evaluation_process(training_set, remaining_df, config)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(f'Evaluation Results: ')\n",
    "    eval_matrix, eval_res = evaluate_results(predictions)\n",
    "\n",
    "    # Save results\n",
    "    save_evaluation_results(training_set, eval_matrix, eval_res, config)\n",
    "\n",
    "def trigger_inference_evaluation_process_k_folds(df, config):\n",
    "    # Select the sample data\n",
    "    folds, remaining_df = create_non_intersecting_sample_sets(df, config['fold_sample_size'], config['folds_num'])\n",
    "    # Compute TF-IDF vectors for the combined text and existing samples\n",
    "    if config['shots_selects_method'] == 'similar':\n",
    "        vectorizer, tfidf_matrix = compute_tfidf_matrix(remaining_df, config['input_fields'])\n",
    "        config['vectorizer'] = vectorizer\n",
    "        config['tfidf_matrix'] = tfidf_matrix\n",
    "    \n",
    "    eval_matrix_list = []\n",
    "    eval_res_list = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        print(f'##################### Fold {i}##########################3')\n",
    "        print(f'Training Data Quality: ')\n",
    "        quality_results = evaluate_dataset_quality(fold, config['input_fields'])\n",
    "        print(quality_results)\n",
    "        \n",
    "        # Start the inference process\n",
    "        print(f'Start the Inference Process ')\n",
    "        predictions = inference_evaluation_process(fold, remaining_df, config)\n",
    "    \n",
    "        # Evaluation\n",
    "        print(f'Evaluation Results: ')\n",
    "        eval_matrix, eval_res = evaluate_results(predictions)\n",
    "        eval_matrix_list.append(eval_matrix)\n",
    "        eval_res_list.append(eval_res)\n",
    "    # # Save results\n",
    "    save_evaluation_results_k_folds(folds, eval_matrix_list, eval_res_list, config)\n",
    "\n",
    "def trigger_inference_evaluation_process(config):\n",
    "    print(config)\n",
    "    # Load the training data\n",
    "    full_training_data = load_full_dataset(config)\n",
    "\n",
    "    # # find valid labels\n",
    "    # config['valid_labels'] = find_valid_labels(full_training_data, elf_df)\n",
    "     \n",
    "    # Check data quality\n",
    "    print(f'Training Data (Full Set) Quality: ')\n",
    "    quality_results = evaluate_dataset_quality(full_training_data, config['input_fields'])\n",
    "    print(quality_results)    \n",
    "\n",
    "    if config['evaluation_method'] == 'single':\n",
    "        trigger_inference_evaluation_process_single(full_training_data, config)\n",
    "    else:\n",
    "        trigger_inference_evaluation_process_k_folds(full_training_data, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423a0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# luminous-base-control-20240215\n",
    "# luminous-extended-control-20240215\n",
    "# luminous-supreme-control-20240215\n",
    "\n",
    "\n",
    "ALPHA_CONFIG = {\n",
    "    'training_data_folder': r'.\\training_data',\n",
    "    'training_data_name': 'preprocessed_training_data_20240521_134540.pkl',\n",
    "    'training_data_size': 10,\n",
    "    'training_data_random_state': 0,\n",
    "    'evaluation_method': 'single', #['folds', 'single']\n",
    "    'folds_num': 5,\n",
    "    'fold_sample_size':300,\n",
    "    'model_type': 'alpha',\n",
    "    \"model_name\" : \"luminous-base\",\n",
    "    \"elf_filter\": ['tech'], #['keywords', 'tech']\n",
    "    \"shots_num\" : 3,\n",
    "    \"shots_selects_method\": 'similar', #['random', 'manual', 'similar']\n",
    "    \"shots_indexes\": [0, 6, 20],\n",
    "    \"random_state\" : 1,\n",
    "    \"max_tokens\": 20,\n",
    "    \"max_prompt_len\": 800,\n",
    "    \"temperature\": 0.1,\n",
    "    \"input_fields\": ['Processed_JobComment', 'Processed_JobSummary'],\n",
    "    \"target_field\": 'ElPred',\n",
    "    'results_folder': r'.\\labelling_results'\n",
    "}\n",
    "\n",
    "MISTRAL_CONFIG = {\n",
    "    'training_data_folder': r'.\\training_data',\n",
    "    'training_data_name': 'preprocessed_training_data_20240521_134540.pkl',\n",
    "    'training_data_size': 300,\n",
    "    'training_data_random_state': 0,\n",
    "    'evaluation_method': 'single', #['folds', 'single']\n",
    "    'folds_num': 2,\n",
    "    'fold_sample_size':100,\n",
    "    'model_type': 'mistral',\n",
    "    \"model_name\" : \"Mistral-7B-Instruct\",\n",
    "    \"elf_filter\": ['tech'], #['keywords', 'tech'], None\n",
    "    \"shots_num\" : 5,\n",
    "    \"shots_selects_method\": 'similar', #['random', 'manual', 'similar']\n",
    "    \"shots_indexes\": [0, 6, 20],\n",
    "    \"random_state\" : 1,\n",
    "    \"max_tokens\": 20,\n",
    "    \"max_prompt_len\": 8000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"input_fields\": ['Processed_JobComment', 'Processed_JobSummary'],\n",
    "    \"target_field\": 'ElPred',\n",
    "    'results_folder': r'.\\labelling_results'\n",
    "}\n",
    "\n",
    "\n",
    "MIXTRAL_CONFIG = {\n",
    "    'training_data_folder': r'.\\training_data',\n",
    "    'training_data_name': 'preprocessed_training_data_20240521_134540.pkl',\n",
    "    'training_data_size': 150,\n",
    "    'training_data_random_state': 0,\n",
    "    'evaluation_method': 'single', #['folds', 'single']\n",
    "    'folds_num': 2,\n",
    "    'fold_sample_size':100,\n",
    "    'model_type': 'mistral',\n",
    "    \"model_name\" : \"Mistral-7B-Instruct\",\n",
    "    \"elf_filter\": ['tech'], #['keywords', 'tech'], None\n",
    "    \"shots_num\" : 5,\n",
    "    \"shots_selects_method\": 'similar', #['random', 'manual', 'similar']\n",
    "    \"shots_indexes\": [0, 6, 20],\n",
    "    \"random_state\" : 1,\n",
    "    \"max_tokens\": 20,\n",
    "    \"max_prompt_len\": 8000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"input_fields\": ['Processed_JobComment', 'Processed_JobSummary'],\n",
    "    \"target_field\": 'ElPred',\n",
    "    'results_folder': r'.\\labelling_results'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207065fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trigger_inference_evaluation_process(MIXTRAL_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec646f7",
   "metadata": {},
   "source": [
    "# Evaluate Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1646d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    'training_data_folder': r'.\\training_data',\n",
    "    'training_data_name': 'preprocessed_training_data_20240521_134540.pkl',\n",
    "    'training_data_size': 150,\n",
    "    'training_data_random_state': 0,\n",
    "    'evaluation_method': 'single', #['folds', 'single']\n",
    "    'folds_num': 2,\n",
    "    'fold_sample_size':100,\n",
    "    'model_type': 'mixtral',\n",
    "    \"model_name\" : \"llama3-70b\",\n",
    "    \"elf_filter\": [], #['keywords', 'tech'], None\n",
    "    \"shots_num\" : 8,\n",
    "    \"shots_selects_method\": 'similar', #['random', 'manual', 'similar']\n",
    "    \"shots_indexes\": [0, 6, 20],\n",
    "    \"random_state\" : 1,\n",
    "    \"max_tokens\": 20,\n",
    "    \"max_prompt_len\": 12000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"input_fields\": ['Processed_JobComment', 'Processed_JobSummary'],\n",
    "    \"target_field\": 'ElPred',\n",
    "    'results_folder': r'.\\labelling_results'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = load_full_dataset(config)\n",
    "quality_results = evaluate_dataset_quality(full_training_data, config['input_fields'])\n",
    "print(quality_results)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(full_training_data['Formatted_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b47fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, remaining_df = select_samples_from_label_groups(full_training_data, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quality_results = evaluate_dataset_quality(training_set, config['input_fields'])\n",
    "print(quality_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc874de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF vectors for the combined text and existing samples\n",
    "if config['shots_selects_method'] == 'similar':\n",
    "    vectorizer, tfidf_matrix = compute_tfidf_matrix(remaining_df, config['input_fields'])\n",
    "    config['vectorizer'] = vectorizer\n",
    "    config['tfidf_matrix'] = tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41eab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference_evaluation_process(training_set, remaining_df, config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_matrix, res_df = evaluate_results(predictions)\n",
    "save_evaluation_results(training_set, eval_matrix, res_df, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80456126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_report(\n",
    "#     res_df['Target_Parent'], \n",
    "#     res_df['Pred_Parent'].fillna(value='not_applicable'), output_dict = True)\n",
    "\n",
    "\n",
    "# def evaluation_matrix(y_true, y_pred):\n",
    "#     matrix = precision_recall_fscore_support(\n",
    "#     res_df['Target_Parent'], \n",
    "#     res_df['Pred_Parent'].fillna(value='not_applicable'),\n",
    "#     labels=list(elf_parents_df[elf_parents_df['Parsed_Label'] != 'probing']['Parsed_Label']))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f2267",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = os.path.join(ALPHA_CONFIG['training_data_folder'], ALPHA_CONFIG['training_data_name'])\n",
    "full_set = pd.read_pickle(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ecec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set[TRAIN_SET_COLS].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc4868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set['Projectnr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_results = evaluate_dataset_quality(full_set, ['Processed_JobSummary', 'Processed_JobComment'], [\"Processed_JobSummary_Lang\", \"Processed_JobComment_Lang\"])\n",
    "quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_full_set = process_dataset(full_set, ALPHA_CONFIG)\n",
    "quality_results = evaluate_dataset_quality(processed_full_set, ['Processed_JobSummary', 'Processed_JobComment'], [\"Processed_JobSummary_Lang\", \"Processed_JobComment_Lang\"])\n",
    "quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_full_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ff70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(processed_full_set['Is_Valid_Label'])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = pd.read_pickle(os.path.join(ALPHA_CONFIG['training_data_folder'], \"preprocessed_unlabelled_data_mc_20240521_134540.pkl\"))\n",
    "quality_results = evaluate_dataset_quality(unlabelled_df, ['Processed_JobSummary', 'Processed_JobComment'], [\"Processed_JobSummary_Lang\", \"Processed_JobComment_Lang\"])\n",
    "quality_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64755801",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled = pd.DataFrame([[label, 0] for label in combined_labels if label not in label_counts.index]).set_index(0)\n",
    "df_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec52bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabelled.columns = pd.DataFrame(label_counts).columns\n",
    "df_unlabelled\n",
    "label_counts_df = pd.concat([pd.DataFrame(label_counts), df_unlabelled], axis=0)\n",
    "label_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = training_set['Formatted_Label'].value_counts()\n",
    "\n",
    "# Print the element counts\n",
    "# print(label_counts)\n",
    "print(len(label_counts))\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "bars = label_counts['count'].plot(kind='bar', color='skyblue')\n",
    "plt.title('ElPred Categories and Counts (Formatted_Label)')\n",
    "plt.xlabel('ElPred Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Add annotations above each bar\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), bar.get_height(), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(processed_full_set['Formatted_Label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdf4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = training_set['Formatted_Label'].value_counts()\n",
    "\n",
    "# Print the element counts\n",
    "# print(label_counts)\n",
    "print(len(label_counts))\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "bars = label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('ElPred Categories and Counts (Formatted label)')\n",
    "plt.xlabel('ElPred Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Add annotations above each bar\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), bar.get_height(), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0692741",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = processed_full_set['ElPred_List'].explode().value_counts()\n",
    "\n",
    "# Print the element counts\n",
    "# print(label_counts)\n",
    "print(len(label_counts))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "bars = label_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('ElPred Label Categories and Counts')\n",
    "plt.xlabel('ElPred Label Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Add annotations above each bar\n",
    "for bar in bars.patches:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), bar.get_height(), ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text =  \" \".join([full_set[field].iloc[0] for field in ALPHA_CONFIG['input_fields']])\n",
    "sample_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer, tfidf_matrix = compute_tfidf_matrix(remaining_set, ALPHA_CONFIG['input_fields'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06decf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = training_set[['Projectnr','Processed_JobComment','Processed_JobSummary', 'ElPred', 'TechGroup']].loc[150]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0782f-6cfd-4450-b0c2-af819a33fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['Processed_JobSummary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10058758",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = ' '.join([sample[col] for col in ALPHA_CONFIG['input_fields']])\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338f45d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_similar_samples(input_text, remaining_set, tfidf_matrix, vectorizer, 5)[['Projectnr','Processed_JobComment','Processed_JobSummary', 'ElPred', 'Similarity']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aca98-45f2-4b42-97ad-bc02c50fa850",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tech_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe8ee7-1c2d-40b3-95fe-899f0e1ca5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(create_prompt_labelling_alpha(processed_training_data.iloc[[0, 6, 15]], 'insert job comment here'))\n",
    "print(create_labelling_prompt_alpha(select_random_samples(processed_full_set, training_config['shots_num'], training_config['random_state'] ),\n",
    "                                    processed_full_set.iloc[0]['Processed_JobSummary'], \n",
    "                                    training_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set[full_set['ElPred']== 'undefined;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_full_set = validate_labels(full_set)\n",
    "valid_full_set[valid_full_set['ElPred']== 'undefined;']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f428418",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set['TechGroup_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_set['TechGroup_2'].info()\n",
    "full_set.dropna(subset=['TechGroup_2']).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae7a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tech_groups()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_elf_by_keywords('scan;scan test;\t', elf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5fb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "elf_parent_df = find_elf_groups(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_parent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_elf_children(graph, ['ElFaultPin'])['Name']\n",
    "find_elf_children(graph,list(elf_parent_df['Name']))['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_elf(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1db8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_df = find_elf_parent_nodes(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_elf_label(parent_df):\n",
    "    combined_df = pd.DataFrame()  \n",
    "    \n",
    "    for group in parent_df['Name']:\n",
    "        children_df = find_elf_children(graph, [group]) \n",
    "        children_df['Parent_Label'] = parent_df.loc[parent_df['Name'] == group, 'Parsed_Label'].iloc[0]\n",
    "        children_df.rename(columns={'Parsed_Label': 'Children_Label'}, inplace=True)\n",
    "        children_df['Combined_Label'] = children_df.apply(lambda row: f\"{row['Parent_Label']};{row['Children_Label']}\", axis=1)\n",
    "        combined_df = pd.concat([combined_df, children_df], ignore_index=True)\n",
    "        combined_df = combined_df[['Parent_Label', 'Children_Label', 'Combined_Label']]\n",
    "    \n",
    "    combined_df = combined_df[['Parent_Label', 'Children_Label', 'Combined_Label']].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    return combined_df\n",
    "\n",
    "# indicate elf_parent_df\n",
    "combined_df = combine_elf_label(elf_parents_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_elf_combined_labels(elf_df):\n",
    "  parents_df = elf_df[elf_df['Type'] == 'Parent']\n",
    "  combined_labels_all = []\n",
    "\n",
    "  for i in range(len(parents_df)):\n",
    "    combined_labels = [parents_df.iloc[i]['Parsed_Label']]\n",
    "    for child in parents_df.iloc[i]['Children']:\n",
    "      combined_labels.append(parents_df.iloc[i]['Parsed_Label']+';'+child)\n",
    "  \n",
    "    combined_labels_all.append(combined_labels)\n",
    "  return combined_labels_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f783b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_labels_groups = find_elf_combined_labels(elf_df)\n",
    "\n",
    "combined_labels = [label for labels in combined_labels_groups for label in labels]\n",
    "len(combined_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6754b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elf_df, elf_parents_df, elf_children_df = load_elf_df(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef344bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8daff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_labelling_prompt_alpha(processed_full_set.head(), elf_df, 'input_text', ALPHA_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd64c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "ALPHA_TOKEN = os.environ.get('ALPHA_TOKEN')\n",
    "emb_url = os.environ.get('EMB_URL')\n",
    "emb_model = \"luminous-base\"\n",
    "payload = json.dumps({\n",
    "  \"model\": emb_model,\n",
    "  \"prompt\": \"An apple a day keeps the doctor away.\",\n",
    "  \"layers\": [\n",
    "    0,\n",
    "    1\n",
    "  ],\n",
    "  \"tokens\": False,\n",
    "  \"pooling\": [\n",
    "    \"max\"\n",
    "  ],\n",
    "  \"type\": \"symmetric\"\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  'Accept': 'application/json',\n",
    "  'cookie': 'token='+ALPHA_TOKEN\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", emb_url, headers=headers, data=payload,  verify=False)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'model_name':'mixtral',\n",
    "    'max_tokens':50,\n",
    "    'temperature': 0.1\n",
    "}\n",
    "mistral_completion('testing', model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_prediction_labels('Parent label: currentconsumption\\n\\\n",
    "Child label: None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df[elf_df['Type'] == 'Child'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcfe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_tech_groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb0d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set[full_set['Projectnr'] == 'MA23AP-02146']\n",
    "# filter_elf_by_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a564ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_elf_by_tech_group('TechGroupMixed', elf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "elf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13eec94",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "    'training_data_folder': r'.\\training_data',\n",
    "    'training_data_name': 'preprocessed_training_data_20240521_134540.pkl',\n",
    "    'training_data_size': 150,\n",
    "    'training_data_random_state': 0,\n",
    "    'evaluation_method': 'single', #['folds', 'single']\n",
    "    'folds_num': 2,\n",
    "    'fold_sample_size':100,\n",
    "    'model_type': 'mixtral',\n",
    "    \"model_name\" : \"gpt-3.5-turbo\",\n",
    "    \"elf_filter\": [], #['keywords', 'tech'], None\n",
    "    \"shots_num\" : 5,\n",
    "    \"shots_selects_method\": 'similar', #['random', 'manual', 'similar']\n",
    "    \"shots_indexes\": [0, 6, 20],\n",
    "    \"random_state\" : 1,\n",
    "    \"max_tokens\": 20,\n",
    "    \"max_prompt_len\": 8000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"input_fields\": ['Processed_JobComment', 'Processed_JobSummary'],\n",
    "    \"target_field\": 'ElPred',\n",
    "    'results_folder': r'.\\labelling_results'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad1b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_data = load_full_dataset(config)\n",
    "training_set, remaining_df = select_samples_from_label_groups(full_training_data, config)\n",
    "# Compute TF-IDF vectors for the combined text and existing samples\n",
    "if config['shots_selects_method'] == 'similar':\n",
    "    vectorizer, tfidf_matrix = compute_tfidf_matrix(remaining_df, config['input_fields'])\n",
    "    config['vectorizer'] = vectorizer\n",
    "    config['tfidf_matrix'] = tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465da4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = training_set.loc[80]\n",
    "sample_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = single_inference(sample_input, elf_df, remaining_df, config)\n",
    "print('Model Response: ', res[0])\n",
    "print('Predicted Lables: ', extract_prediction_labels(res[0]))\n",
    "print('Prompt: ', res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter elf\n",
    "filtered_elf_df = filter_elf_by_tech_group(sample_input['Parsed_TechGroup'], elf_df)\n",
    "filtered_elf_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb173485",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_labelling_prompt_mistral(remaining_df, filtered_elf_df, 'input_text', config, True)\n",
    "mixtral_completion(prompt, config), prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75ca84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
